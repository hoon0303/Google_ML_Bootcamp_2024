# Neural Networks and Deep Learning

## 신경망이란 무엇인가?

- **신경망**: 인간의 두뇌 신경세포의 구조와 기능을 모방한 컴퓨팅 시스템.
- **뉴런**: 신경망의 기본 단위. 입력값을 받아 가중치와 바이어스를 적용하여 출력값을 생성.
- **가중치(Weights)**: 입력 데이터의 중요도를 조절하는 값.
- **바이어스(Bias)**: 뉴런이 활성화되는 기준을 조절하는 값.
- **레이어**: 입력층, 은닉층, 출력층으로 구성. 각 층은 여러 뉴런으로 이루어짐.
- **입력층**: 데이터가 신경망으로 들어오는 곳.
- **은닉층**: 입력 데이터를 처리하여 특성(feature)을 추출하는 층.
- **출력층**: 최종 예측을 제공하는 층.

## 신경망을 사용한 지도 학습

- **지도 학습**: 라벨이 있는 데이터셋을 사용하여 학습. 입력 데이터와 그에 상응하는 출력 데이터를 사용.
- **목표**: 입력 데이터에 대한 정확한 예측을 학습.
- **학습 과정**: 입력 데이터를 신경망에 입력 -> 예측 결과 도출 -> 실제 값과 비교하여 오차 계산 -> 오차를 최소화하도록 가중치와 바이어스 조정.

## 딥러닝이 떠오르는 이유

- **빅 데이터**: 대규모 데이터의 이용 가능성.
- **강력한 하드웨어**: GPU와 같은 강력한 하드웨어의 발전.
- **효과적인 알고리즘**: 새로운 알고리즘과 최적화 기법의 개발.
- **병렬 처리**: GPU를 활용한 병렬 처리로 대규모 신경망의 효율적인 학습 가능.
- **알고리즘 혁신**: 딥러닝 알고리즘의 발전으로 더 복잡하고 정확한 모델 생성 가능.

## 이진 분류

- **이진 분류**: 두 가지 클래스 중 하나로 데이터를 분류하는 문제.
- **예**: 스팸 이메일 감지(스팸/비스팸).

## 로지스틱 회귀

- **로지스틱 회귀**: 이진 분류를 위한 통계적 모델.
- **시그모이드 함수**: 출력값을 0과 1 사이로 변환. \( \sigma(z) = \frac{1}{1 + e^{-z}} \), where \( z = w^T x + b \).
- <img src="https://latex.codecogs.com/png.latex?\dpi{300}&space;\sigma(z)&space;=&space;\frac{1}{1&space;&plus;&space;e^{-z}},&space;\text{where}&space;z&space;=&space;w^T&space;x&space;&plus;&space;b" alt="로지스틱 회귀의 시그모이드 함수">

## 로지스틱 회귀 비용 함수

- **비용 함수**: 모델의 예측과 실제 값 간의 차이를 측정. 로지스틱 회귀에서는 교차 엔트로피 비용 함수를 사용. J(θ) = -1/m Σ [ y(i) log(hθ(x(i))) + (1 - y(i)) log(1 - hθ(x(i))) ]

## 그라데이션 하강

- **그라데이션 하강**: 비용 함수를 최소화하기 위해 사용되는 최적화 알고리즘. θ := θ - α ∂J(θ)/∂θ
- **학습률(α)**: 매 반복에서의 가중치 업데이트 크기.

## 파생상품 및 계산 그래프

- **파생상품**: 함수의 변화율을 나타냄. 학습 과정에서 그라데이션 계산에 사용.
- **계산 그래프**: 함수의 계산 과정을 그래프로 표현. 역전파 알고리즘에 유용.
- **순전파**: 입력에서 출력으로의 계산.
- **역전파**: 출력에서 입력으로의 그라데이션 계산.

## 로지스틱 회귀의 그라데이션 하강

- **벡터화**: 여러 데이터 샘플을 한꺼번에 처리하여 계산 효율성 증가.
- **벡터화된 그라데이션 계산**: 행렬 연산을 통해 모든 샘플의 그라데이션을 동시에 계산.

## 신경망 개요 및 표현

- **신경망 구조**: 여러 층의 뉴런으로 구성. 각 층의 출력은 다음 층의 입력으로 사용.
- **출력 계산**: 입력 데이터에 가중치와 바이어스를 적용하고 활성화 함수를 통해 출력값을 계산. a[l] = g(z[l]) = g(W[l]a[l-1] + b[l])

## 활성화 함수

- **활성화 함수**: 뉴런의 출력을 비선형적으로 변환. 대표적인 함수로 시그모이드, ReLU 등이 있음.
- **ReLU**: Rectified Linear Unit. g(z) = max(0, z)
- **비선형성 필요성**: 비선형 활성화 함수를 사용하여 신경망이 복잡한 패턴을 학습할 수 있게 함.

## 신경망을 위한 그라데이션 하강 및 역전파

- **역전파 알고리즘**: 출력층에서 입력층으로 그라데이션을 계산하여 가중치를 업데이트.
- **역전파 과정**: 출력에서 입력으로 이동하면서 그라데이션을 계산하고, 가중치와 바이어스를 업데이트.
- **무작위 초기화**: 가중치의 초기값을 무작위로 설정하여 대칭성 문제를 방지.

## 심층 L-레이어 신경망

- **심층 신경망**: 다층 신경망으로, 은닉층의 수가 많을수록 더 복잡한 패턴을 학습할 수 있음.
- **순방향 전파**: 입력 데이터를 통해 각 층의 출력을 순차적으로 계산.
- **역방향 전파**: 출력층에서부터 입력층까지 그라데이션을 계산하여 가중치를 업데이트.

## 매개변수 대 하이퍼파라미터

- **매개변수**: 모델이 학습하는 값(예: 가중치, 바이어스).
- **하이퍼파라미터**: 학습 과정에서 설정하는 값(예: 학습률, 은닉층 수).
- **튜닝**: 하이퍼파라미터를 조정하여 모델의 성능을 최적화.
