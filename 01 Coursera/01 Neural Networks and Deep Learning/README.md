# Neural Networks and Deep Learning

This course is the first in the Deep Learning specialization offered by [Coursera](https://www.coursera.org/specializations/deep-learning) and moderated by [DeepLearning.ai](http://deeplearning.ai). The course is taught by Andrew Ng.

## Course Summary

This course serves as an introduction to deep learning. You will learn:
- The major technology trends driving Deep Learning.
- How to build, train, and apply fully connected deep neural networks.
- How to implement efficient (vectorized) neural networks.
- Key parameters in a neural network's architecture.

## Table of contents


* [Introduction to Deep Learning](#introduction-to-deep-learning)
   * [What is a (Neural Network) NN?](#what-is-a-neural-network-nn)
   * [Supervised learning with neural networks](#supervised-learning-with-neural-networks)
   
* [Neural Networks Basics](#neural-networks-basics)
   * [Binary classification](#binary-classification)
   * [Logistic regression](#logistic-regression)
   * [Logistic regression cost function](#logistic-regression-cost-function)
   * [Gradient Descent](#gradient-descent)
   * [Derivatives](#derivatives)
   * [More Derivatives examples](#more-derivatives-examples)
   * [Computation graph](#computation-graph)
   * [Derivatives with a Computation Graph](#derivatives-with-a-computation-graph)
   * [Logistic Regression Gradient Descent](#logistic-regression-gradient-descent)
   * [Gradient Descent on m Examples](#gradient-descent-on-m-examples)
   * [Vectorization](#vectorization)
   * [Vectorizing Logistic Regression](#vectorizing-logistic-regression)
   * [Notes on Python and NumPy](#notes-on-python-and-numpy)
     
* [Shallow neural networks](#shallow-neural-networks)
   * [Neural Networks Overview](#neural-networks-overview)
   * [Neural Network Representation](#neural-network-representation)
   * [Computing a Neural Network's Output](#computing-a-neural-networks-output)
   * [Vectorizing across multiple examples](#vectorizing-across-multiple-examples)
   * [Activation functions](#activation-functions)
   * [Why do you need non-linear activation functions?](#why-do-you-need-non-linear-activation-functions)
   * [Derivatives of activation functions](#derivatives-of-activation-functions)
   * [Gradient descent for Neural Networks](#gradient-descent-for-neural-networks)
   * [Random Initialization](#random-initialization)
* [Deep Neural Networks](#deep-neural-networks)
   * [Deep L-layer neural network](#deep-l-layer-neural-network)
   * [Forward Propagation in a Deep Network](#forward-propagation-in-a-deep-network)
   * [Getting your matrix dimensions right](#getting-your-matrix-dimensions-right)
   * [Why deep representations?](#why-deep-representations)
   * [Building blocks of deep neural networks](#building-blocks-of-deep-neural-networks)
   * [Forward and Backward Propagation](#forward-and-backward-propagation)
   * [Parameters vs Hyperparameters](#parameters-vs-hyperparameters)
      
   
